{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 20:45:42.484024: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-09 20:45:42.602749: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-09 20:45:42.602772: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-09 20:45:42.632556: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-09 20:45:43.318072: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-09 20:45:43.318173: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-09 20:45:43.318184: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/ubuntu/miniconda3/envs/base_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tempus_fugit/flatblack/benchmarking/tensorflow\n"
     ]
    }
   ],
   "source": [
    "ROOT = os.path.realpath(\"..\")\n",
    "print(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased bert_base_uncased /home/ubuntu/tempus_fugit/flatblack/benchmarking/tensorflow/models/bert/\n",
      "assets\tsaved_model.pb\tvariables\n"
     ]
    }
   ],
   "source": [
    "# Set model environment variables\n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MODEL_BASE_PATH = f'{ROOT}/models/bert/'\n",
    "\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME\n",
    "os.environ['MODEL_NAME_UNDER'] = re.sub(\"-\",\"_\", MODEL_NAME)\n",
    "os.environ['MODEL_BASE_PATH'] = MODEL_BASE_PATH\n",
    "\n",
    "! echo $MODEL_NAME $MODEL_NAME_UNDER $MODEL_BASE_PATH \n",
    "! cd $MODEL_BASE_PATH && ls\n",
    "! echo \"export MODEL_BASE_PATH=$MODEL_BASE_PATH\" >> ~/.bash_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 20:48:50.788328: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-09 20:48:50.788360: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-09 20:48:50.788381: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-25-33): /proc/driver/nvidia/version does not exist\n",
      "2022-12-09 20:48:50.788642: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/tempus_fugit/flatblack/benchmarking/tensorflow/models/bert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ubuntu/tempus_fugit/flatblack/benchmarking/tensorflow/models/bert/assets\n"
     ]
    }
   ],
   "source": [
    "# Download example model - in this case bert-base-uncased\n",
    "model = TFBertModel.from_pretrained(MODEL_NAME)\n",
    "# Save model in SavedModel Format\n",
    "tf.saved_model.save(model,MODEL_BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find image 'tensorflow/serving:latest' locally\n",
      "latest: Pulling from tensorflow/serving\n",
      "\n",
      "\u001b[1Be5416296: Pulling fs layer \n",
      "\u001b[1B1bab055f: Pulling fs layer \n",
      "\u001b[1B6ae4c4ea: Pulling fs layer \n",
      "\u001b[1B66225230: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:e66c1866bf6596473d56beb3ef77bf7b8b00baa0e1e80c3c428ede31da8ae066[2A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for tensorflow/serving:latest\n",
      "aeb73c0d0a1b5d87ebb1135da1fffa77b6027cca9d38e9ed8b9acaae0974b202\n"
     ]
    }
   ],
   "source": [
    "# Build a tensorflow-serve docker image\n",
    "#! docker pull tensorflow/serving\n",
    "#! docker images | grep tensorflow/serving\n",
    "! docker run -d --name tf_serving_base tensorflow/serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tempus_fugit/flatblack/benchmarking/tensorflow/models/bert/\n"
     ]
    }
   ],
   "source": [
    "! echo $MODEL_BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Copy model into  tensorflow serving container\n",
    "! cd $MODEL_BASE_PATH && ls\n",
    "! docker cp $MODEL_BASE_PATH tf_serving_base:/models/$MODEL_NAME_UNDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tempus_fugit/flatblack/benchmarking/tensorflow/models/bert/\n",
      "ENV MODEL_NAME -base-uncased\n"
     ]
    }
   ],
   "source": [
    "!echo $MODEL_BASE_PATH\n",
    "!echo ENV MODEL_NAME ${MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:8c7f9527d570a800257014b64978980cee2cd4461d154f7ddc0163b088d74170\n"
     ]
    }
   ],
   "source": [
    "# Commit the container to serving the model \n",
    "! docker commit --change \"ENV MODEL_NAME ${MODEL_NAME_UNDER}\" tf_serving_base $USER/tf_serving_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_serving_base\n",
      "tf_serving_base\n"
     ]
    }
   ],
   "source": [
    "# Kill tf_serving base \n",
    "! docker kill tf_serving_base\n",
    "! docker rm tf_serving_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker images | grep $USER/tf_serving_base\n",
    "\n",
    "# Start the container with our model\n",
    "! docker run -p 8500:8500 -t $USER/tf_serving_base &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-09 22:28:33.267495: I tensorflow_serving/model_servers/server.cc:74] Building single TensorFlow model file config:  model_name: bert_base_uncased model_base_path: /models/bert_base_uncased\n",
      "2022-12-09 22:28:33.267723: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.\n",
      "2022-12-09 22:28:33.267748: I tensorflow_serving/model_servers/server_core.cc:594]  (Re-)adding model: bert_base_uncased\n",
      "2022-12-09 22:28:33.459775: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: bert_base_uncased version: 1}\n",
      "2022-12-09 22:28:33.459822: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: bert_base_uncased version: 1}\n",
      "2022-12-09 22:28:33.459843: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: bert_base_uncased version: 1}\n",
      "2022-12-09 22:28:33.459892: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /models/bert_base_uncased/1\n",
      "2022-12-09 22:28:33.563682: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-12-09 22:28:33.563729: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /models/bert_base_uncased/1\n",
      "2022-12-09 22:28:33.563838: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-09 22:28:33.815965: I external/org_tensorflow/tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2022-12-09 22:28:33.877280: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-12-09 22:28:34.649477: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /models/bert_base_uncased/1\n",
      "2022-12-09 22:28:34.979165: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 1519269 microseconds.\n",
      "2022-12-09 22:28:35.043370: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:62] No warmup data file found at /models/bert_base_uncased/1/assets.extra/tf_serving_warmup_requests\n",
      "2022-12-09 22:28:35.138072: I tensorflow_serving/core/loader_harness.cc:95] Successfully loaded servable version {name: bert_base_uncased version: 1}\n",
      "2022-12-09 22:28:35.140946: I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models\n",
      "2022-12-09 22:28:35.141009: I tensorflow_serving/model_servers/server.cc:118] Using InsecureServerCredentials\n",
      "2022-12-09 22:28:35.141035: I tensorflow_serving/model_servers/server.cc:383] Profiler service is enabled\n",
      "2022-12-09 22:28:35.142244: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "2022-12-09 22:28:35.144698: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n"
     ]
    }
   ],
   "source": [
    "!docker run -p 8501:8501 \\\n",
    "    -v \"${MODEL_BASE_PATH}:/models/${MODEL_NAME_UNDER}\" \\\n",
    "    -e MODEL_NAME=$MODEL_NAME_UNDER \\\n",
    "    -t $USER/tf_serving_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tempus_fugit/flatblack/benchmarking/tensorflow/models/bert/:/models/bert_base_uncased\n"
     ]
    }
   ],
   "source": [
    "! echo \"${MODEL_BASE_PATH}:/models/${MODEL_NAME_UNDER}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('base_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e7279dc53c325d580ff14194516b4068e7e02df947a022dba79c8b4269df8be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
